q_fct_config = {"hidden_layers": [96, 96]}
v_fct_config = {"hidden_layers": [96, 96]}
pi_fct_config = {"hidden_layers": [96, 96], "dim":3}

SAC_agent = SoftActorCritic(o_space=o_space, 
                            a_space=ac_space, 
                            value_fct=Value_function, 
                            policy_fct=Normal_Policy, 
                            env= env,
                            discount=  0.95, 
                            tau = 0.005, 
                            batch_size=64, 
                            alpha = 0.7,
                            lambda_V = 3e-4, 
                            lambda_Pi = 3e-4, 
                            lambda_Q = 3e-4,
                            dim_act = 3,
                            dim_obs = 16,
                            buffer_size = int(3e5),
                            q_fct_config=q_fct_config,
                            v_fct_config=v_fct_config,
                            pi_fct_config=pi_fct_config, 
                            save_path = save_path_new) 

reward =  reward  + _info['reward_closeness_to_puck']  +  _info['reward_touch_puck']
train(10000, burn_in = 0, writer = writer)