{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC.Normal_Policy import Normal_Policy\n",
    "from SAC.Value_function import Value_function\n",
    "from SAC.SoftActorCritc import SoftActorCritic, plot\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './../../laser-hockey-env')\n",
    "import laser_hockey_env as lh\n",
    "from importlib import reload\n",
    "\n",
    "import progressbar\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Laser_hockey\"\n",
    "env = lh.LaserHockeyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1.]\n",
      "Box(16,)\n"
     ]
    }
   ],
   "source": [
    "ac_space = env.action_space\n",
    "o_space = env.observation_space\n",
    "print(ac_space.low)\n",
    "print(o_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"C:/Users/Maximilian/Documents/Studium/Informatik/3. Semester/Intelligent Systems/project/DeepRLHockey/code/SAC/weights/\"\n",
    "version = \"v_full_10\"\n",
    "save_path_new = direct+version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_fct_config = {\"hidden_layers\": [128, 128],\"weights_init\":None,\"bias_init\" :None}\n",
    "v_fct_config = {\"hidden_layers\": [128, 128],\"weights_init\":None,\"bias_init\" :None}\n",
    "pi_fct_config = {\"hidden_layers\": [128, 128], \"dim\":3,\"weights_init\":None,\"bias_init\" :None}\n",
    "\n",
    "SAC_agent = SoftActorCritic(o_space=o_space, \n",
    "                            a_space=ac_space, \n",
    "                            value_fct=Value_function, \n",
    "                            policy_fct=Normal_Policy, \n",
    "                            env= env,\n",
    "                            discount=  0.92, \n",
    "                            tau = 0.01, \n",
    "                            batch_size=128, \n",
    "                            alpha = 'auto',\n",
    "                            lambda_V = 3e-4, \n",
    "                            lambda_Pi = 3e-4, \n",
    "                            lambda_Q = 3e-4,\n",
    "                            dim_act = 3,\n",
    "                            dim_obs = 16,\n",
    "                            buffer_size = int(1e5),\n",
    "                            q_fct_config=q_fct_config,\n",
    "                            v_fct_config=v_fct_config,\n",
    "                            pi_fct_config=pi_fct_config, \n",
    "                            save_path = save_path_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "player2 = lh.BasicOpponent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('visualization/{}-{}'.format(env_name, version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(iter_fit=1000, max_steps= 500,  env_steps = 1, grad_steps = 1, burn_in =5, writer = None):\n",
    "        start = SAC_agent._sess.run(SAC_agent.global_step)\n",
    "        saver = SAC_agent._saver\n",
    "        bar = progressbar.ProgressBar(max_value=iter_fit)\n",
    "        # Initilalize target V network\n",
    "        SAC_agent._sess.run(SAC_agent._update_target_V_ops_hard)\n",
    "        # Init Statistics      \n",
    "          #moving_avg_winner =np.zeros(20)\n",
    "        winner_count = 0\n",
    "        draw_count = 0\n",
    "        total = 0\n",
    "        \n",
    "        j = start\n",
    "        for i in range(iter_fit):\n",
    "            \n",
    "            ob = env.reset()\n",
    "            \n",
    "            obs_agent2 = env.obs_agent_two()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            #puck_hit = 0.0\n",
    "            #puck_hit_once = False\n",
    "            for _ in range(max_steps):\n",
    "                for e_i in range(env_steps):#\n",
    "                    \n",
    "                    #env.render()\n",
    "                    if j < burn_in:\n",
    "                        a = env.action_space.sample()\n",
    "                        a = a[:3]\n",
    "                        #a = player2.act(ob) \n",
    "                           \n",
    "                                        \n",
    "                    else:\n",
    "                        #print(SAC_agent.act_greedy(np.asarray(ob).reshape(1, SAC_agent._config[\"dim_obs\"])))\n",
    "                        a = SAC_agent.action(np.asarray(ob).reshape(1, SAC_agent._config[\"dim_obs\"]))\n",
    "                        a = a[0]\n",
    "                    a2 = player2.act(obs_agent2)                    \n",
    "                    comb_a = np.hstack([a,a2])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    (ob_new, reward, done, _info) = env.step(comb_a)\n",
    "                     \n",
    "                    #reward =  reward  + _info['reward_closeness_to_puck']  +  _info['reward_touch_puck']\n",
    "                    reward = reward - (_info['reward_puck_direction'] == 0.0) * 0.01 #+ _info['reward_puck_direction']\n",
    "                    total_reward += reward\n",
    "                    \n",
    "                    #SAC_agent.store_transition(ob, a, reward, ob_new, done)\n",
    "                    SAC_agent.buffer.store(ob, a, reward, ob_new, done)\n",
    "                    ob=ob_new\n",
    "                    obs_agent2 = env.obs_agent_two()\n",
    "  \n",
    "                if j  >= 2:\n",
    "                    for g_i in range(grad_steps):\n",
    "                        update_value_target = False\n",
    "                        if i % SAC_agent._config[\"target_update\"] == 0:\n",
    "                            update_value_target = True\n",
    "                        loss_V_fct, loss_Q1_fct,loss_Q2_fct,loss_PI_fct= SAC_agent._train(update_value_target)\n",
    "\n",
    "                        \n",
    "                \n",
    "                if done:\n",
    "                    break \n",
    "            #moving_avg_winner[i%20]=((_info['winner']+1)/2.0)/20\n",
    "            j += 1\n",
    "            winner_count +=  _info['winner'] == 1\n",
    "            draw_count += _info['winner'] == 0\n",
    "            total += 1              \n",
    "            SAC_agent._sess.run(SAC_agent.increment_global_step_op)\n",
    "            #total_rewards_per_episode.append(total_reward)\n",
    "            #if i % 1 == 0:\n",
    "            if i % 250 == 0:\n",
    "                saver.save(SAC_agent._sess, save_path_new)\n",
    "\n",
    "                #plot(total_rewards_per_episode,winning=winner, plot_type = 3) \n",
    "            if writer: \n",
    "                writer.add_scalar('win_loss_ratio/wins',winner_count/total , start+i)\n",
    "                writer.add_scalar('win_loss_ratio/draws',draw_count/total , start+i)\n",
    "                writer.add_scalar('reward',total_reward , start+i)\n",
    "            bar.update(i)   \n",
    "        return winner_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (4 of 10000) |                      | Elapsed Time: 0:00:20 ETA:  19:56:11"
     ]
    }
   ],
   "source": [
    "train(10000, burn_in = 0, writer = writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Maximilian/Documents/Studium/Informatik/3. Semester/Intelligent Systems/project/DeepRLHockey/code/SAC/weights/v_full_9'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAC_agent._saver.save(SAC_agent._sess, save_path_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    ob = env.reset()\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "#        distance_to_opponent = np.sqrt((ob[6] -ob[0])**2 + (ob[7] -ob[1])**2)\n",
    " #       distance_to_puck = np.sqrt((ob[12] -ob[0])**2 + (ob[13] -ob[1])**2)\n",
    "  #      angle_to_opponent = np.arctan2((ob[7] -ob[1]),(ob[6] -ob[0])) - ob[2]\n",
    "   #     angle_to_puck = np.arctan2((ob[13] -ob[1]),(ob[12] -ob[0])) - ob[2]\n",
    "    #    ob = np.append(ob, [distance_to_opponent, distance_to_puck,angle_to_opponent, angle_to_puck] )\n",
    "        obs_agent2 = env.obs_agent_two()    \n",
    "        a = SAC_agent.action(np.asarray(ob).reshape(1, SAC_agent._config[\"dim_obs\"]))\n",
    "        a1 = a[0]\n",
    "        #print(a1, SAC_agent.act_greedy(np.asarray(ob).reshape(1, SAC_agent._config[\"dim_obs\"]))[0])\n",
    "        #a1 = player2.act(ob)\n",
    "        \n",
    "        a2 = player2.act(obs_agent2)\n",
    "        \n",
    "        ob, r, d, info = env.step(np.hstack([a1,a2]))    \n",
    "        #print(info)\n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "        if d: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
