{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC.Normal_Policy import Normal_Policy\n",
    "from SAC.Value_function import Value_function\n",
    "from SAC.SoftActorCritc import SoftActorCritic, plot\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './../../laser-hockey-env')\n",
    "import laser_hockey_env as lh\n",
    "from importlib import reload\n",
    "\n",
    "import progressbar\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Laser_hockey\"\n",
    "env = lh.LaserHockeyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1.]\n",
      "Box(16,)\n"
     ]
    }
   ],
   "source": [
    "ac_space = env.action_space\n",
    "o_space = env.observation_space\n",
    "print(ac_space.low)\n",
    "print(o_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"C:/Users/Maximilian/Documents/Studium/Informatik/3. Semester/Intelligent Systems/project/DeepRLHockey/code/SAC/weights/\"\n",
    "version = \"v_full_9\"\n",
    "save_path_new = direct+version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_fct_config = {\"hidden_layers\": [128, 128],\"weights_init\":None,\"bias_init\" :None}\n",
    "v_fct_config = {\"hidden_layers\": [128, 128],\"weights_init\":None,\"bias_init\" :None}\n",
    "pi_fct_config = {\"hidden_layers\": [128, 128], \"dim\":3,\"weights_init\":None,\"bias_init\" :None}\n",
    "\n",
    "SAC_agent = SoftActorCritic(o_space=o_space, \n",
    "                            a_space=ac_space, \n",
    "                            value_fct=Value_function, \n",
    "                            policy_fct=Normal_Policy, \n",
    "                            env= env,\n",
    "                            discount=  0.92, \n",
    "                            tau = 0.01, \n",
    "                            batch_size=128, \n",
    "                            alpha = 2.0,\n",
    "                            lambda_V = 3e-4, \n",
    "                            lambda_Pi = 3e-4, \n",
    "                            lambda_Q = 3e-4,\n",
    "                            dim_act = 3,\n",
    "                            dim_obs = 16,\n",
    "                            buffer_size = int(1e5),\n",
    "                            q_fct_config=q_fct_config,\n",
    "                            v_fct_config=v_fct_config,\n",
    "                            pi_fct_config=pi_fct_config, \n",
    "                            save_path = save_path_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "player2 = lh.BasicOpponent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('visualization/{}-{}'.format(env_name, version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(iter_fit=1000, max_steps= 500,  env_steps = 1, grad_steps = 1, burn_in =5, writer = None):\n",
    "        start = SAC_agent._sess.run(SAC_agent.global_step)\n",
    "        saver = SAC_agent._saver\n",
    "        bar = progressbar.ProgressBar(max_value=iter_fit)\n",
    "        # Initilalize target V network\n",
    "        SAC_agent._sess.run(SAC_agent._update_target_V_ops_hard)\n",
    "        # Init Statistics      \n",
    "          #moving_avg_winner =np.zeros(20)\n",
    "        winner_count = 0\n",
    "        draw_count = 0\n",
    "        total = 0\n",
    "        \n",
    "        j = start\n",
    "        for i in range(iter_fit):\n",
    "            \n",
    "            ob = env.reset()\n",
    "            \n",
    "            obs_agent2 = env.obs_agent_two()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            #puck_hit = 0.0\n",
    "            #puck_hit_once = False\n",
    "            for _ in range(max_steps):\n",
    "                for e_i in range(env_steps):#\n",
    "                    \n",
    "                    #env.render()\n",
    "                    if j < burn_in:\n",
    "                        a = env.action_space.sample()\n",
    "                        a = a[:3]\n",
    "                        #a = player2.act(ob) \n",
    "                           \n",
    "                                        \n",
    "                    else:\n",
    "                        #print(SAC_agent.act_greedy(np.asarray(ob).reshape(1, SAC_agent._config[\"dim_obs\"])))\n",
    "                        a = SAC_agent.action(np.asarray(ob).reshape(1, SAC_agent._config[\"dim_obs\"]))\n",
    "                        a = a[0]\n",
    "                    a2 = player2.act(obs_agent2)                    \n",
    "                    comb_a = np.hstack([a,a2])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    (ob_new, reward, done, _info) = env.step(comb_a)\n",
    "                     \n",
    "                    #reward =  reward  + _info['reward_closeness_to_puck']  +  _info['reward_touch_puck']\n",
    "                    reward = reward - (_info['reward_puck_direction'] == 0.0) * 0.01 #+ _info['reward_puck_direction']\n",
    "                    total_reward += reward\n",
    "                    \n",
    "                    #SAC_agent.store_transition(ob, a, reward, ob_new, done)\n",
    "                    SAC_agent.buffer.store(ob, a, reward, ob_new, done)\n",
    "                    ob=ob_new\n",
    "                    obs_agent2 = env.obs_agent_two()\n",
    "  \n",
    "                if j  >= 2:\n",
    "                    for g_i in range(grad_steps):\n",
    "                        update_value_target = False\n",
    "                        if i % SAC_agent._config[\"target_update\"] == 0:\n",
    "                            update_value_target = True\n",
    "                        loss_V_fct, loss_Q1_fct,loss_Q2_fct,loss_PI_fct= SAC_agent._train(update_value_target)\n",
    "\n",
    "                        \n",
    "                \n",
    "                if done:\n",
    "                    break \n",
    "            #moving_avg_winner[i%20]=((_info['winner']+1)/2.0)/20\n",
    "            j += 1\n",
    "            winner_count +=  _info['winner'] == 1\n",
    "            draw_count += _info['winner'] == 0\n",
    "            total += 1              \n",
    "            SAC_agent._sess.run(SAC_agent.increment_global_step_op)\n",
    "            #total_rewards_per_episode.append(total_reward)\n",
    "            #if i % 1 == 0:\n",
    "            if i % 250 == 0:\n",
    "                saver.save(SAC_agent._sess, save_path_new)\n",
    "\n",
    "                #plot(total_rewards_per_episode,winning=winner, plot_type = 3) \n",
    "            if writer: \n",
    "                writer.add_scalar('win_loss_ratio/wins',winner_count/total , start+i)\n",
    "                writer.add_scalar('win_loss_ratio/draws',draw_count/total , start+i)\n",
    "                writer.add_scalar('reward',total_reward , start+i)\n",
    "            bar.update(i)   \n",
    "        return winner_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16% (1621 of 10000) |###                | Elapsed Time: 2:46:39 ETA:  23:18:37"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8b77a6c5df2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mburn_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-9f543fa698f4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(iter_fit, max_steps, env_steps, grad_steps, burn_in, writer)\u001b[0m\n\u001b[0;32m     61\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mSAC_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target_update\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                             \u001b[0mupdate_value_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                         \u001b[0mloss_V_fct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_Q1_fct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_Q2_fct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_PI_fct\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mSAC_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_value_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Studium\\Informatik\\3. Semester\\Intelligent Systems\\project\\DeepRLHockey\\code\\SAC\\SoftActorCritc.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, update_value_target)\u001b[0m\n\u001b[0;32m    233\u001b[0m                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_opQ2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ2_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                      self._update_target_V_ops]\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_PI_fct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_V_fct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_Q1_fct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_Q2_fct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfddct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maximilian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maximilian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maximilian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maximilian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maximilian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maximilian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(10000, burn_in = 0, writer = writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Maximilian/Documents/Studium/Informatik/3. Semester/Intelligent Systems/project/DeepRLHockey/code/SAC/weights/v_full_9'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAC_agent._saver.save(SAC_agent._sess, save_path_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    ob = env.reset()\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "#        distance_to_opponent = np.sqrt((ob[6] -ob[0])**2 + (ob[7] -ob[1])**2)\n",
    " #       distance_to_puck = np.sqrt((ob[12] -ob[0])**2 + (ob[13] -ob[1])**2)\n",
    "  #      angle_to_opponent = np.arctan2((ob[7] -ob[1]),(ob[6] -ob[0])) - ob[2]\n",
    "   #     angle_to_puck = np.arctan2((ob[13] -ob[1]),(ob[12] -ob[0])) - ob[2]\n",
    "    #    ob = np.append(ob, [distance_to_opponent, distance_to_puck,angle_to_opponent, angle_to_puck] )\n",
    "        obs_agent2 = env.obs_agent_two()    \n",
    "        a = SAC_agent.action(np.asarray(ob).reshape(1, SAC_agent._config[\"dim_obs\"]))\n",
    "        a1 = a[0]\n",
    "        #print(a1, SAC_agent.act_greedy(np.asarray(ob).reshape(1, SAC_agent._config[\"dim_obs\"]))[0])\n",
    "        #a1 = player2.act(ob)\n",
    "        \n",
    "        a2 = player2.act(obs_agent2)\n",
    "        \n",
    "        ob, r, d, info = env.step(np.hstack([a1,a2]))    \n",
    "        #print(info)\n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "        if d: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
